
from __future__ import absolute_import, division, print_function

from chunk import Chunk
import os
import json
import numpy as np
from typing import Any, Callable, Iterable, List, Text, Tuple, Union
import zlib

from wholecell.utils import filepath
from . import tablewriter as tw
from six.moves import zip

__all__ = [
	"TableReader",
	"TableReaderError",
	"VersionError",
	"DoesNotExistError",
	"VariableLengthColumnError",
	]

SUPPORTED_COMPRESSION_TYPES = (tw.COMPRESSION_TYPE_NONE, tw.COMPRESSION_TYPE_ZLIB)
SUBCOLUMNS_KEY = "subcolumns"


class TableReaderError(Exception):
	"""
	Base exception class for TableReader-associated exceptions.
	"""
	pass


class VersionError(TableReaderError):
	"""
	An error raised when the input files claim to be from a different format or
	version of the file specification.
	"""
	pass


class DoesNotExistError(TableReaderError):
	"""
	An error raised when a column or attribute does not seem to exist.
	"""
	pass


class VariableLengthColumnError(TableReaderError):
	"""
	An error raised when the user tries to access subcolumns of a variable
	length column.
	"""
	pass


class _ColumnHeader(object):
	'''Column header info read from a Column file's first chunk.'''
	def __init__(self, chunk):
		# type: (Chunk) -> None
		chunk_name = chunk.getname()
		if chunk_name != tw.COLUMN_CHUNK_TYPE and chunk_name != tw.VARIABLE_COLUMN_CHUNK_TYPE:
			raise VersionError('Not a supported Column file format/version')

		self.variable_length = chunk_name == tw.VARIABLE_COLUMN_CHUNK_TYPE

		if self.variable_length:
			header_struct = chunk.read(tw.VARIABLE_COLUMN_STRUCT.size)
			(self.compression_type, ) = tw.VARIABLE_COLUMN_STRUCT.unpack(header_struct)

		else:
			header_struct = chunk.read(tw.COLUMN_STRUCT.size)
			(self.bytes_per_entry,
			self.elements_per_entry,
			self.entries_per_block,
			self.compression_type) = tw.COLUMN_STRUCT.unpack(header_struct)

		if self.compression_type not in SUPPORTED_COMPRESSION_TYPES:
			raise VersionError('Unsupported Column compression type {}'.format(
				self.compression_type))

		descr_json = chunk.read()
		descr = json.loads(descr_json)

		if isinstance(descr, (str, Text)):
			# really the dtype.descr
			self.dtype = str(descr)  # type: Union[str, List[Tuple[str, str]]]
		else:
			# numpy requires list-of-tuples-of-strings
			# TODO(jerry): Support triples?
			self.dtype = [(str(n), str(t)) for n, t in descr]


class TableReader(object):
	"""
	Reads output generated by TableWriter.

	Parameters:
		path (str): Path to the input location (a directory).

	See also
	--------
	wholecell.io.tablewriter.TableWriter
	wholecell.tests.io.measure_bulk_reader
	wholecell.tests.io.measure_zlib
	"docs/misc/byte_strings_to_2D_arrays.md"
	"""

	def __init__(self, path):
		# type: (str) -> None
		self._path = path

		# Read the table's attributes file
		attributes_filename = os.path.join(path, tw.FILE_ATTRIBUTES)
		try:
			self._attributes = filepath.read_json_file(attributes_filename)

		except IOError as e:
			raise VersionError(
				"Could not read a table's attributes file ({})."
				" Version 2 tables are not supported."  # they could be...
				" Unzip all table files if needed.".format(attributes_filename), e)

		# Check if the table's version matches the expected version
		version = self._attributes['_version']
		if version != tw.VERSION:
			raise VersionError("Expected version {} but found version {}".format(
				tw.VERSION, version))

		# List the column file names. Ignore the 'attributes.json' file.
		self._columnNames = {p for p in os.listdir(path) if '.json' not in p}

	@property
	def path(self):
		# type: () -> str
		return self._path


	def readAttribute(self, name):
		# type: (str) -> Any
		"""
		Return an attribute value.

		Parameters:
			name: The attribute name.

		Returns:
			value: The attribute value, JSON-deserialized from a string.
		"""

		if name not in self._attributes:
			raise DoesNotExistError("No such attribute: {}".format(name))
		return self._attributes[name]


	def readColumn(self, name, indices=None, squeeze=True):
		# type: (str, Any, bool) -> np.ndarray
		"""
		Load a full column (all rows). Each row entry is a 1-D NumPy array of
		subcolumns, so the initial result is a 2-D array row x subcolumn, which
		is optionally squeezed to arrays with lower dimensions if squeeze=True.
		In the case of fixed-length columns, this method can optionally read
		just a vertical slice of all those arrays -- the subcolumns at the
		given `indices`. For variable-length columns, np.nan is used as a
		filler value for the empty entries of each row.

		The current approach collects up the compressed blocks, allocates the
		result array, then unpacks entries into it, keeping each decompressed
		block in memory only while copying into the result array. For a large
		column, this saves considerable RAM and a bit of time over collecting
		decompressed blocks then combining them via np.vstack().

		See docs/misc/byte_strings_to_2D_arrays.md for more design tradeoffs
		and their performance measurements.

		Parameters:
			name: The name of the column.
			indices: The subcolumn indices to select from each entry. This can
				be any value that works to index an ndarray along 1 dimension,
				or None for all the data. Specifying this argument
				for variable-length columns will throw an error.

				If provided, this can give a performance boost for columns that
				are wide and tall.

				NOTE: The speed benefit might only be realized if the file is
				in the disk cache (i.e. the file has been recently read), which
				should typically be the case. This will still save RAM.
			squeeze: If True, the resulting NumPy array is squeezed into a 0D,
				1D, or 2D array, depending on the number of rows and subcolumns
				it has.
				1 row x 1 subcolumn => 0D.
				n rows x 1 subcolumn or 1 row x m subcolumns => 1D.
				n rows x m subcolumns => 2D.

		Returns:
			ndarray: A writable 0D, 1D, or 2D array.

		TODO (jerry): Bring back the code to block-read `indices` of the data
			from uncompressed tables or after decompression, via seek + read or
			np.frombuffer(data, dtype, count, offset). It might be worthwhile
			only when header.entries_per_block == 1.

			The speed of various read methods is surprising and shape dependent.
			Techniques like `frombuffer(join(all_the_bytestrings))` or loop
			over `result[i, :] = frombuffer(byestring)` tended to take about as
			long in a simple test of BulkMolecules/counts but run slower in
			measure_bulk_reader.py. The differences are more pronounced for a
			smaller table like BulkMolecules/atpRequested.
		"""
		def decomp(raw_block):
			# type: (bytes) -> np.ndarray
			'''Decompress and unpack a raw block to an ndarray.'''
			data = decompressor(raw_block)

			if variable_length:
				entries_ = np.frombuffer(data, header.dtype)
			else:
				entries_ = np.frombuffer(data, header.dtype).reshape(
					-1, header.elements_per_entry)
				if indices is not None:
					entries_ = entries_[:, indices]

			return entries_

		if name not in self._columnNames:
			raise DoesNotExistError("No such column: {}".format(name))

		entry_blocks = []  # type: List[bytes]
		row_size_blocks = []

		# Read the header and read, decompress, and unpack all the blocks.
		with open(os.path.join(self._path, name), 'rb') as dataFile:
			chunk = Chunk(dataFile, align=False)
			header = _ColumnHeader(chunk)
			variable_length = header.variable_length
			chunk.close()

			if variable_length and indices is not None:
				raise VariableLengthColumnError(
					'Attempted to access subcolumns of a variable-length column {}.'.format(name))

			# Variable-length columns should not be squeezed.
			if variable_length:
				squeeze = False

			if header.compression_type == tw.COMPRESSION_TYPE_ZLIB:
				decompressor = lambda data_bytes: zlib.decompress(data_bytes)  # type: Callable[[bytes], bytes]
			else:
				decompressor = lambda data_bytes: data_bytes

			while True:
				try:
					chunk = Chunk(dataFile, align=False)
				except EOFError:
					break

				if chunk.getname() == tw.BLOCK_CHUNK_TYPE:
					raw_entry = chunk.read()
					if len(raw_entry) != chunk.getsize():
						raise EOFError('Data block cut short {}/{}'.format(
							len(raw_entry), chunk.getsize()))
					entry_blocks.append(raw_entry)

				elif chunk.getname() == tw.ROW_SIZE_CHUNK_TYPE:
					row_sizes = chunk.read()
					if len(row_sizes) != chunk.getsize():
						raise EOFError('Row sizes block cut short {}/{}'.format(
							len(row_sizes), chunk.getsize()))
					row_size_blocks.append(row_sizes)

				chunk.close()  # skips to the next chunk

		if variable_length and len(entry_blocks) != len(row_size_blocks):
			raise EOFError('Number of entry blocks ({}) does not match number of row size blocks ({}).'.format(
				len(entry_blocks), len(row_size_blocks)))

		del raw_entry  # release the block ref

		# Variable-length columns
		if variable_length:
			# Concatenate row sizes array
			row_sizes_list = [
				np.frombuffer(block, tw.ROW_SIZE_CHUNK_DTYPE)
				for block in row_size_blocks]  # type: List[Iterable[int]]
			all_row_sizes = np.concatenate(row_sizes_list)

			# Initialize results array to NaNs
			result = np.full((len(all_row_sizes), all_row_sizes.max()), np.nan)

			row = 0
			for raw_entry, row_sizes_ in zip(entry_blocks, row_sizes_list):
				entries = decomp(raw_entry)
				entry_idx = 0

				# Fill each row with the length given by values in row_sizes_
				for row_size in row_sizes_:
					result[row, :row_size] = entries[entry_idx : (entry_idx + row_size)]
					entry_idx += row_size
					row += 1

		# Constant-length columns
		else:
			# Decompress the last block to get its shape, then allocate the result.
			last_entries = decomp(entry_blocks.pop())
			last_num_rows = last_entries.shape[0]
			num_rows = len(entry_blocks) * header.entries_per_block + last_num_rows
			num_subcolumns = header.elements_per_entry if indices is None else len(indices)
			result = np.zeros((num_rows, num_subcolumns), header.dtype)

			row = 0
			for raw_entry in entry_blocks:
				entries = decomp(raw_entry)
				additional_rows = entries.shape[0]
				result[row : (row + additional_rows)] = entries
				row += additional_rows

			result[row : (row + last_num_rows)] = last_entries

		# Squeeze if flag is set to True
		if squeeze:
			result = result.squeeze()

		return result


	def readSubcolumn(self, column, subcolumn_name):
		# type: (str, str) -> np.ndarray
		"""Read in a subcolumn from a table by name

		Each column of a table is a 2D matrix. The SUBCOLUMNS_KEY attribute
		defines a map from column name to a name for an attribute that
		stores a list of names such that the i-th name describes the i-th
		subcolumn.

		Arguments:
			column: Name of the column.
			subcolumn_name: Name of the ID or object associated with the
				desired subcolumn.

		Returns:
			The subcolumn, as a 1-dimensional array.
		"""
		subcol_name_map = self.readAttribute(SUBCOLUMNS_KEY)
		subcols = self.readAttribute(subcol_name_map[column])
		index = subcols.index(subcolumn_name)
		return self.readColumn(column, [index], squeeze=False)[:, 0]


	def allAttributeNames(self):
		"""
		Returns a list of all attribute names including Table metadata.
		"""
		return list(self._attributes.keys())


	def attributeNames(self):
		"""
		Returns a list of ordinary (client-provided) attribute names.
		"""
		names = [key for key in self._attributes if not key.startswith('_')]
		return names


	def columnNames(self):
		"""
		Returns the names of all columns.
		"""
		return list(self._columnNames)


	def close(self):
		"""
		Does nothing.

		The TableReader keeps no files open, so this method does nothing.

		Notes
		-----
		TODO (John): Consider removing this method.  At the moment are usage is
			inconsistent, and gives the impression that it is actually
			beneficial or necessary.
		"""
		pass
